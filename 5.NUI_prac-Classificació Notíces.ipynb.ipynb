{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctiques de Nous Usos de la Informàtica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nom de les persones del grup:** Mr X & Ms. Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pràctica 3: Naive Bayes i Classificació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Què s’ha de fer?</b><br>\n",
    "Volem classificar notícies corresponents al diari New York Times segons de quin tòpic parlin principalment. A partir  de totes les notíces que tenim guardades en un fitxer .csv, crearem un vector de característiques que ens descrigui  cada notícia. Finalment desenvoluparem un classificador probabilístic del tipus Naive Bayes que ens permeti identificar a quin tòpic pertany una notícia donada segons les característiques triades.<br>\n",
    "\n",
    "<b>Quina és la idea del sistema de classificació que s’ha de desenvolupar?</b><br>\n",
    "L'objectiu del classificador és, donat un vector de característiques que descriuen els objectes que es volen classificar, indicar a quina categoria o classe pertanyen d'entre un conjunt predeterminat. El procés de classificació consta de dues parts:\n",
    "\n",
    "+ el procés d'aprenentatge \n",
    "+ el procés d'explotació o testeig. \n",
    "\n",
    "El procés d'aprenentatge rep exemples de parelles $(x,y)$ on $x$ són les característiques, usualment nombres reals, i $y$ és la categoria a la que pertanyen. Aquest conjunt se'l coneix com a conjunt d'entrenament i ens servirà per trobar una funció $y=h(x)$ que donada una $x$ m'indiqui quina és la $y$. Per altra banda el procés de testeig aplica la funció $h(x)$ apresa a l'entrenament a una nova descripció per veure quina categoria li correspon.</br>\n",
    "\n",
    "<b>Classificació i llenguatge natural</b><br>\n",
    "La descripció dels exemples en característiques és el punt més crític de tot sistema d'aprenentatge automàtic. Una de les representacions més simples per tal de descriure un text és la representació bag-of-words. Aquesta representació converteix un text en un vector de $N$ paraules. Consisteix en seleccionar un conjunt d'$N$ paraules i per cada paraula contar quants cops apareix en el text. Una versió alternativa d'aquest procés pot ser simplement indicar si apareix o no en el text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dades del New York Times</b>\n",
    "En aquest exemple, el nostre objectiu és classificar automàticament les notícies d'acord al seu titular en vint temes o tòpics. Les dades que disposem són cadascú dels articles de primera plana del New York Times entre 1996 i 2006, classificats segons Policy Agendas (http://www.policyagendas.org ). Aquesta recollida de dades l'ha compilat n'Amber E. Boydstun.\n",
    "\n",
    "Concretament, els tòpics són els següents\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>\n",
    "1 \n",
    "<td>\n",
    "Macroeconomics\n",
    "<tr>\n",
    "<td>\n",
    "2 \n",
    "<td>\n",
    "Civil Rights, Minority Issues, and Civil Liberties \n",
    "<tr>\n",
    "<td>\n",
    "3\n",
    "<td>\n",
    "Health\n",
    "<tr>\n",
    "<td>\n",
    "4 \n",
    "<td>Agriculture\n",
    "<tr>\n",
    "<td>\n",
    "5 \n",
    "<td>Labor, Employment, and Immigration\n",
    "<tr>\n",
    "<td>\n",
    "6 \n",
    "<td> Education\n",
    "<tr>\n",
    "<td>\n",
    "7\n",
    "<td>Environment\n",
    "<tr>\n",
    "<td>\n",
    "8\n",
    "<td>Energy\n",
    "<tr>\n",
    "<td>\n",
    "10 \n",
    "<td>Transportation\n",
    "<tr>\n",
    "<td>\n",
    "12 \n",
    "<td>Law, Crime, and Family Issues\n",
    "<tr>\n",
    "<td>\n",
    "13 \n",
    "<td>Social Welfare\n",
    "<tr>\n",
    "<td>\n",
    "14 \n",
    "<td>Community Development and Housing Issues\n",
    "<tr>\n",
    "<td>\n",
    "15 \n",
    "<td>Banking, Finance, and Domestic Commerce\n",
    "<tr>\n",
    "<td>\n",
    "16 \n",
    "<td>Defense\n",
    "<tr>\n",
    "<td>\n",
    "17 \n",
    "<td>Space, Science, Technology and Communications\n",
    "<tr>\n",
    "<td>\n",
    "18 \n",
    "<td>Foreign Trade\n",
    "<tr>\n",
    "<td>\n",
    "19 \n",
    "<td>International Affairs and Foreign Aid\n",
    "<tr>\n",
    "<td>\n",
    "20 \n",
    "<td>Government Operations\n",
    "<tr>\n",
    "<td>\n",
    "21 \n",
    "<td>Public Lands and Water Management\n",
    "<tr>\n",
    "<td>\n",
    "24 \n",
    "<td>State and Local Government Administration\n",
    "<tr>\n",
    "<td>\n",
    "26 \n",
    "<td>Weather and Natural Disasters\n",
    "<tr>\n",
    "<td>\n",
    "27 \n",
    "<td>Fires\n",
    "<tr>\n",
    "<td>\n",
    "28 \n",
    "<td>Arts and Entertainment\n",
    "<tr>\n",
    "<td>\n",
    "29 \n",
    "<td>Sports and Recreation\n",
    "<tr>\n",
    "<td>\n",
    "30 \n",
    "<td>Death Notices\n",
    "<tr>\n",
    "<td>\n",
    "31 \n",
    "<td>Churches and Religion\n",
    "<tr>\n",
    "<td>\n",
    "99 \n",
    "<td>Other, Miscellaneous, and Human Interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exemple.</b>\n",
    "Imaginem que tenim 4 notícies que pertanyen només a dues categories ``y={'8. Energy', '2. Civil Rights, Minority Issues, and Civil Liberties'}``, podríem seleccionar les següents paraules per tal de distingir les dues categories ``x={'nuclear', 'verd', 'ecologia', 'independència', 'autonomia', 'referèndum'}``.\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td></td>\n",
    "<td>nuclear</td>\n",
    "<td>verd</td>\n",
    "<td>ecologia</td>\n",
    "<td>independència</td>\n",
    "<td>autonomia</td>\n",
    "<td>referèndum</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 1('2. Civil Rights...')</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 2('2. Civil Rights...')</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 3('8. Energy')</td>\n",
    "<td>1</td>\n",
    "<td>3</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 4('8. Energy')</td>\n",
    "<td>2</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "amb aquesta representació la notícia 2, corresponent a la categoria ``'2. Civil Rights...'``, quedaria representat pel vector numèric ``(0,1,0,1,2,3)``. Si fem servir la representació alternativa (booleana) tindríem ``(0,1,0,1,1,1)`` que indica la presència de les paraules. Si la descripció és adient s'espera que les categories es puguin distingir entre elles amb facilitat.\n",
    "<br><br>\n",
    "<b>El classificador Naïve Bayes.</b><br>\n",
    "Un cop tenim una representació necessitem un procés d'aprenentatge que ens permeti passar de la descripció a una categoria. En aquesta pràctica farem servir el classificador Naïve Bayes. Aquest classificador forma part de la família de classificadors probabilístics. La sortida d'un classificador probabilístic és un valor de probabilitat donat un exemple per cadascuna de les categories. La decisió final correspon a la categoria amb més probabilitat. Per exemple, amb la descripció anterior esperem que la sortida sigui de l'estil,<br>\n",
    "\n",
    "$$p( y = 'Civil\\_Rights' \\; | \\; x = (0,1,0,1,1,1)) = 0.6$$\n",
    "$$p( y = 'Energy' \\; | \\; x = (0,1,0,1,1,1)) = 0.4$$\n",
    "\n",
    "<br>\n",
    "Els classificadors probabilistics Bayesians es basen en el teorema de Bayes per realitzar els càlculs per trobar la probabilitat condicionada: <br>\n",
    "\n",
    "$$ p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "<br>\n",
    "d'on podem extreure que: <br>\n",
    "$$ p(y,x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "En molts casos p(y) i p(x) són desconeguts i es consideren equiprobables. Per tant, la\n",
    "decisió es simplifica a:\n",
    "<br>\n",
    "$$ p(y|x) = c · p(x|y)$$\n",
    "\n",
    "<br>\n",
    "Les deduccions fins a aquest punt són vàlides per la majoria de classificadors Bayesians. Naïve Bayes es distingeix de la resta perquè imposa una condició encara més restrictiva. Considerem $x=(x_1,x_2,x_3,...,x_N)$ un conjunt d'$N$ variables aleatòries. Naïve Bayes assumeix que totes elles són independents entre elles i per tant podem escriure:\n",
    "<br>\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y)$$\n",
    "\n",
    "<br>\n",
    "Per tant en el nostre cas es pot veure com:\n",
    "<br>\n",
    "$$p(y='Civil\\_Rights' \\; |\\;x=(0,1,0,1,1,1)) = p(x_1=0|\\;'Civil\\_Rights' \\; )p(x_2=1|\\;'Civil\\_Rights' \\; )...p(x_6=1|\\;'Civil\\_Rights' \\; )$$\n",
    "\n",
    "<br>\n",
    "Podem interpretar l'anterior equació de la següent forma: La probabilitat de que el document descrit pel vector de característiques (0,1,0,1,1,1) sigui de la classe \"Civil Rights\" és proporcional al producte de la probabilitat que la primera paraula del vector (nuclear) no aparegui en les notícies sobre \"Civil Rights\"  per la probabilitat que la segona paraula sí que hi aparegui, etc.\n",
    "\n",
    "<br>\n",
    "<b>Estimant les probabilitats marginals condicionades</b>\n",
    "L'últim pas que ens queda és trobar el valor de les probabilitats condicionades. Farem servir la representació de 0's i 1's indicant que la paraula no apareix (0) o sí apareix (1) a la notícia. Per trobar el valor de la probabilitat condicionada farem servir una aproximació freqüentista a la probabilitat. Això vol dir que calcularem la freqüència d'aparició de cada paraula per a cada categoria. Aquest càlcul es fa dividint el nombre de notícies de la categoria en que apareix la paraula pel nombre total de notícies d'aquella categoria. En l'exemple anterior, \n",
    "$p('verd'=1 |y='Civil\\_Rights')=1/2 $, mentre que  $p('verd' =1 |y='Energy')=2/2 $\n",
    "\n",
    "En general:\n",
    "<br>\n",
    "$$p(x = 1 | y = C)= \\frac{A}{B} $$\n",
    "<br>\n",
    "on A és el número de notícies de la categoria C on hi apareix la paraula 'x' i B és el número total de notícies de la categoria C.\n",
    "\n",
    "\n",
    "<b>Punts dèbils: </b><br>\n",
    "\n",
    "<b> * El problema de la probabilitat 0</b>\n",
    "Si us hi fixeu bé, en l'anterior exemple la probabilitat <b>p('verd'= 0 | y='Energy')</b> és 0 !! Això vol dir, que si en la notícia no hi apareix la paraula 'verd' no pot ser classificada com a categoria 'Energy'!! No sembla raonable que s'assigni o no en aquesta categoria segons si en la notícia hi apareix o no una única paraula. Per tant, el que s'acostuma a fer és donar una baixa probabilitat en comptes de zero. Una de les possibles solucions es fer servir la correcció de Laplace. Seguint l'exemple anterior la correcció de Laplace és\n",
    "\n",
    "<br>\n",
    "$$p(x=1 | y = 'C' ) = \\frac{A+1}{B+M}$$ \n",
    "on M és el nombre de catergories\n",
    "\n",
    "<b> * El problema de com escollir el vector de carecterístiques</b>\n",
    "L'elecció de les paraules que formen el vector de característiques és un pas crític. En funció de com de bona sigui aquesta descripció, millor funcionarà el sistema. Tot i que us deixem a vosaltres la política de creació del vector de característiques us donem una d'exemple. Per saber quines paraules fer servir podeu seleccionar de totes les paraules de totes les notícies aquelles que apareixen entre en un 10 i un 50 percent del total (sense tenir en compte la categoria). Podeu experimentar variant aquests valors.\n",
    "\n",
    "<b> * El problema del \"underflow\"</b>\n",
    " La funció que hem de calcular en el Naive Bayes és un producte. El nombre de caractéristiques del vector és el nombre de termes del producte. Aquests nombres són iguals o menors a 1, si els multipliquem tots entre ells el resultat serà massa petit per a representar-lo en un nombre de punt flotant i el càlcul acabarà sent reduït a zero. Per solucionar aquest problema en comptes d'operar fent multiplicacions, se sol passar a l'escala logarítmica i allà operar fent servir sumes en comptes de multiplicacions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Article_Sequence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Topic_6digit</th>\n",
       "      <th>Topic_4digit</th>\n",
       "      <th>Topic_2digit</th>\n",
       "      <th>War on Terror</th>\n",
       "      <th>Katrina</th>\n",
       "      <th>Israel/Palestine</th>\n",
       "      <th>Immigration</th>\n",
       "      <th>Presidential Elections</th>\n",
       "      <th>Clinton Impeachment</th>\n",
       "      <th>Enron</th>\n",
       "      <th>Darfur</th>\n",
       "      <th>Race/Ethnicity</th>\n",
       "      <th>Schiavo</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>a</td>\n",
       "      <td>Nation's Smaller Jails Struggle To Cope With S...</td>\n",
       "      <td>Jails overwhelmed with hardened criminals</td>\n",
       "      <td>120500</td>\n",
       "      <td>1205</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nation's Smaller Jails Struggle To Cope With S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>b</td>\n",
       "      <td>Dancing (and Kissing) In the New Year</td>\n",
       "      <td>new years activities</td>\n",
       "      <td>280000</td>\n",
       "      <td>2800</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dancing (and Kissing) In the New Year new year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>c</td>\n",
       "      <td>Forbes's Silver Bullet for the Nation's Malaise</td>\n",
       "      <td>Steve Forbes running for President</td>\n",
       "      <td>201201</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Forbes's Silver Bullet for the Nation's Malais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>d</td>\n",
       "      <td>Up at Last, Bridge to Bosnia Is Swaying Gatewa...</td>\n",
       "      <td>U.S. military constructs bridge to help their ...</td>\n",
       "      <td>160200</td>\n",
       "      <td>1602</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Up at Last, Bridge to Bosnia Is Swaying Gatewa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>e</td>\n",
       "      <td>2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...</td>\n",
       "      <td>Democrats and Republicans can't agree on plan ...</td>\n",
       "      <td>201206</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date Article_Sequence  \\\n",
       "Article_ID                              \n",
       "1           1/1/1996                a   \n",
       "2           1/1/1996                b   \n",
       "3           1/1/1996                c   \n",
       "4           1/1/1996                d   \n",
       "5           1/1/1996                e   \n",
       "\n",
       "                                                        Title  \\\n",
       "Article_ID                                                      \n",
       "1           Nation's Smaller Jails Struggle To Cope With S...   \n",
       "2                      Dancing (and Kissing) In the New Year    \n",
       "3            Forbes's Silver Bullet for the Nation's Malaise    \n",
       "4           Up at Last, Bridge to Bosnia Is Swaying Gatewa...   \n",
       "5           2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...   \n",
       "\n",
       "                                                      Summary  Topic_6digit  \\\n",
       "Article_ID                                                                    \n",
       "1                   Jails overwhelmed with hardened criminals        120500   \n",
       "2                                        new years activities        280000   \n",
       "3                          Steve Forbes running for President        201201   \n",
       "4           U.S. military constructs bridge to help their ...        160200   \n",
       "5           Democrats and Republicans can't agree on plan ...        201206   \n",
       "\n",
       "            Topic_4digit  Topic_2digit  War on Terror  Katrina  \\\n",
       "Article_ID                                                       \n",
       "1                   1205            12              0        0   \n",
       "2                   2800            28              0        0   \n",
       "3                   2012            20              0        0   \n",
       "4                   1602            16              0        0   \n",
       "5                   2012            20              0        0   \n",
       "\n",
       "            Israel/Palestine  Immigration  Presidential Elections  \\\n",
       "Article_ID                                                          \n",
       "1                          0            0                       0   \n",
       "2                          0            0                       0   \n",
       "3                          0            0                       1   \n",
       "4                          0            0                       0   \n",
       "5                          0            0                       0   \n",
       "\n",
       "            Clinton Impeachment  Enron  Darfur  Race/Ethnicity  Schiavo  \\\n",
       "Article_ID                                                                \n",
       "1                             0      0       0               0        0   \n",
       "2                             0      0       0               0        0   \n",
       "3                             0      0       0               0        0   \n",
       "4                             0      0       0               0        0   \n",
       "5                             0      0       0               0        0   \n",
       "\n",
       "                                                          new  \n",
       "Article_ID                                                     \n",
       "1           Nation's Smaller Jails Struggle To Cope With S...  \n",
       "2           Dancing (and Kissing) In the New Year new year...  \n",
       "3           Forbes's Silver Bullet for the Nation's Malais...  \n",
       "4           Up at Last, Bridge to Bosnia Is Swaying Gatewa...  \n",
       "5           2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "#load data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv',index_col=\"Article_ID\")\n",
    "data[\"new\"] = data[\"Title\"].map(str) + data[\"Summary\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 1 </b> Escriure una funció <b>count_news(dataframe)</b> que retorni el nombre total de noticies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31034\n"
     ]
    }
   ],
   "source": [
    "#Retorna el número total de notícies. \n",
    "def count_news(df):\n",
    "    return len(df.index)\n",
    "\n",
    "print count_news(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 2:</b> Escriure una funció <b>count_topic_news(dataframe)</b> que compti quantes notícies hi ha per cadascun dels tópics. Agaferem el camp \"Topic_2digit\" com a identificador de tópic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 964, 2: 914, 3: 1799, 4: 168, 5: 749, 6: 912, 7: 354, 8: 299, 10: 594, 12: 2088, 13: 273, 14: 410, 15: 1249, 16: 4479, 17: 719, 18: 254, 19: 6354, 20: 3958, 21: 269, 24: 715, 26: 573, 27: 129, 28: 769, 29: 1273, 30: 268, 31: 329, 99: 172}\n"
     ]
    }
   ],
   "source": [
    "#Retorna una Series que compte el número de notícies per a cadascun dels tópics\n",
    "def count_topic_news(df):\n",
    "    temp=df['Topic_2digit']\n",
    "    count_dicc={}\n",
    "    for i in temp:\n",
    "        if(i in count_dicc):\n",
    "            count_dicc[i]+=1\n",
    "        else:\n",
    "            count_dicc[i]=1\n",
    "    return count_dicc\n",
    "\n",
    "print count_topic_news(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 3:</b> Escriure una funció <b>count_words(dataframe)</b> que retorni un diccionari amb totes les paraules que hi hagi en totes les notícies, indicant per cada paraula quantes ocurrències en total hi ha i en quantes notícies surt.\n",
    "<br>Possible format sortida: {word :  {n_ocur: valor;n_news: valor}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aquesta funció ha de construir un diccionari que contingui totes les paraules que s'han trobat indicant \n",
    "# el total de cops que ha aparegut i el nombre de notícies on apareix\n",
    "def count_words(df):\n",
    "    word_dicc={}\n",
    "    last=-1\n",
    "    for i,row in df.iterrows():\n",
    "        w=''\n",
    "        for c in row['new']:\n",
    "            if(c!=' '):\n",
    "                if c.isalnum():\n",
    "                    w+=c.lower()\n",
    "            elif w!='':\n",
    "                if w in word_dicc:\n",
    "                    word_dicc[w]['n_ocur']+=1\n",
    "                    if last!=i:\n",
    "                        last=i\n",
    "                        word_dicc[w]['n_news']+=1\n",
    "                else:\n",
    "                    word_dicc[w]={'n_ocur': 1,'n_news': 1}\n",
    "                w=''\n",
    "        last=i\n",
    "    return word_dicc\n",
    "\n",
    "dicc_text = count_words(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 4:</b> escriure una funció <b>count_words_topic(dataframe,dicc_text)</b> que retorna un diccionari que conté el nombre de cops que ha aparegut  cada paraula i el número de notícies on  ha aparegut. Aquesta informació ha de ser dividida pels diferents tópics de noticies.\n",
    "<br>Possible format sortida: {Topic :  {word :  {n_ocur: valor;n_news: valor} } } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compta la freqüència de les paraules per a un tòpic determinat\n",
    "def count_words_topic(df):\n",
    "    temp=df['Topic_2digit']\n",
    "    count_dicc={}\n",
    "    for i in temp:\n",
    "        if(i not in count_dicc):\n",
    "            count_dicc[i]=count_words(df.loc[df['Topic_2digit']==i])\n",
    "    return count_dicc             \n",
    "\n",
    "words_topics=count_words_topic(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 5:</b> Calcular amb la funció <b>topNword(df,words_topics,N)</b> quines son les N paraules més representatives (les que apareixen amb més freqüència) de cadascun dels tòpics. Retorneu un diccionari amb els següent format: {1: llista_top_words_topic_1; 2: llista_top_words_topic_2;...}\n",
    "<br>Teniu en compte que també haureu de filtrar aquelles paraules que apareixen en la majoria de notícies, així com també, les que únicament apareixen en un conjunt molt petit de notícies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ['stocks', 'fed', 'economic', 'budget', 'federal'],\n",
       " 2: ['abortion', 'gay', 'supreme', 'blacks', 'black'],\n",
       " 3: ['medicare', 'health', 'fda', 'scientists', 'tobacco'],\n",
       " 4: ['farmers', 'mad', 'food', 'meat', 'may'],\n",
       " 5: ['labor', 'immigrants', 'union', 'transit', 'cuban'],\n",
       " 6: ['schools', 'school', 'college', 'chancellor', 'students'],\n",
       " 7: ['epa', 'air', 'global', 'water', 'warming'],\n",
       " 8: ['oil', 'power', 'energy', 'blackout', 'con'],\n",
       " 10: ['mta', 'airlines', 'faa', 'air', 'taxi'],\n",
       " 12: ['officer', 'andersen', 'enron', 'texas', 'gun'],\n",
       " 13: ['welfare', 'social', 'most', 'rules', 'living'],\n",
       " 14: ['housing', 'homeless', 'plan', 'now', 'by'],\n",
       " 15: ['microsoft', 'enron', 'billion', 'wall', 'enrons'],\n",
       " 16: ['nato', 'military', '911', 'army', 'rumsfeld'],\n",
       " 17: ['nasa', 'space', 'shuttle', 'tv', 'technology'],\n",
       " 18: ['trade', 'chinese', 'talks', 'global', 'asian'],\n",
       " 19: ['suicide', 'russians', 'chinese', 'japan', 'british'],\n",
       " 20: ['impeachment', 'gore', 'cheney', 'reno', 'gingrich'],\n",
       " 21: ['over', 'water', 'central', 'memorial', 'plan'],\n",
       " 24: ['bloomberg', 'race', 'albany', 'budget', 'governor'],\n",
       " 26: ['hurricane', 'storm', 'earthquake', 'cold', 'toll'],\n",
       " 27: ['fire', 'killed', 'blaze', 'die', 'wildfires'],\n",
       " 28: ['art', 'not', 'arts', 'book', 'out'],\n",
       " 29: ['yankees', 'pro', 'knicks', 'atlanta', 'world'],\n",
       " 30: ['dies', 'john', 'master', 'crash', 'george'],\n",
       " 31: ['pope', 'vatican', 'cardinals', 'christmas', 'cardinal'],\n",
       " 99: ['editors', 'that', 'town', 'over', 'millionaires']}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calcula les N parules més representativa de cada tòpic . La sortida ha de \n",
    "# ser un diccionari on tenim tantes entrades com tòpics\n",
    "# el valors de les entrades ha de ser una llista amb les paraules seleccionades.\n",
    "import operator\n",
    "def topNwords(df,words_topics,N):\n",
    "    top_words={}\n",
    "    #Most common words in the news\n",
    "    news=[key for key, value in sorted(count_words(data).items(), key=operator.itemgetter(1), reverse=True)[:100]]  \n",
    "    #Most common words in English\n",
    "    english=['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at']\n",
    "    #To be able to change the set fast\n",
    "    repeated=news\n",
    "    for i in words_topics:\n",
    "        if(i not in top_words):\n",
    "            top_words[i]=[key for key, value in sorted(words_topics[i].items(), key=operator.itemgetter(1), reverse=True) if key not in repeated][:N]\n",
    "    return top_words\n",
    "topNwords(data,words_topics,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 6</b>: Creeu el vector de característiques necessari per a fer l’entrenament del Naïve Bayes (funció <b>create_features()</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crea el vector de característiques necessari per a l'entrenament del classificador Naive Bayes\n",
    "# selected_words: ha de ser el diccionari que retorna topNWords.\n",
    "# train_data : conté totes les notícies del conjunt d'entrenament\n",
    "# Rertorna un diccionari que conté un np.array per a cadascuna de les notícies amb el vector de característiques corresponent.\n",
    "\n",
    "def create_features(train_data,selected_words):\n",
    "    dict_feat_vector=dict()\n",
    "    return dict_feat_vector\n",
    "\n",
    "N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor. \n",
    "words_topics=count_words_topic(data,dicc_text)\n",
    "top_words=topNwords(data,words_topics,N)\n",
    "dict_feat_vector = create_features(data,top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 7</b>: Implementeu la funció d'aprenentatge del classificador Naïve Bayes (funció <b>naive_bayes_learn()</b>).  La funció ha de mostrar per pantalla el resultat obtingut \n",
    "<br>\n",
    "<b> * L'error d'entrenament</b>\n",
    "L'error d'entrenament es troba calculant el percentatge d'errors que s'obtenen quan es fa el testeig amb les mateixes dades utilizades per fer entrenament (aprenentatge). Aquest error es un valor molt optimista de com funcionarà el clasificador i mai s'ha de prendre com a mesura per comparar clasificadors.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mètode que implementa el clasificador Naive_Bayes.Ha de mostrar el resultat obtingut per pantalla\n",
    "def naive_bayes(df,feature_vector):\n",
    "    return 0\n",
    "\n",
    "## EXEMPLE SORTIDA (LES XIFRES SÓN INVENTADES!):\n",
    "#Tòpic 1 encerts: 24 / Notícies: 34 / 70.59 %\n",
    "#Tòpic 2  encerts: 21 / Notícies: 26 / 80.77 %\n",
    "#Tòpic 3  encerts: 29 / Notícies: 32 / 90.62 %\n",
    "#Tòpic 4  encerts: 26 / Notícies: 29 / 89.66 %\n",
    "#\n",
    "#Error naive bayes: 17.36 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 8: </b> Indiqueu l'error de generalització fent servir *n-fold validation* (funció <b>n_fold ()</b> )\n",
    "\n",
    "**Aproximació a l'error de generalització fent servir  *n-fold validation* **.\n",
    "Una bona forma de veure com funcionaria el nostre classificador davant de dades sobre les quals no s'ha entrenat és fer servir l'estratègia n-fold. Aquesta estratègia testeja el classificador amb una partició de les dades d'entrenament i fa l'entrenament sobre la resta de dades que hem exclòs. Aquest procés d'exclusió es repeteix per cadascún dels *folds* de les dades d'entrenament. El nombre de *folds* determina quantes particions hem de fer, i per tant, les dades que hi ha en el conjut de test. Per exemple, en un *5-fold* validation, es fan 5 particions, les dades de test són un cinquè de les dades i l'entrenament es fa amb els quatre cinquens restants. El percentatge d'errors fent servir aquesta estratègia permet comparar classificadors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mètode per avaluar el classificador mitjançant la tècnica n-fold validation. n és el nombre de folds  \n",
    "#Ha de mostrar per pantalla el resultat obtingut \n",
    "def n_fold(df,feature_vector,n):\n",
    "     return 0\n",
    "    \n",
    "## EXEMPLE SORTIDA (LES XIFRES SÓN INVENTADES!):\n",
    "#Tòpic 1 encerts: 24 / Notícies: 34 / 70.59 %\n",
    "#Tòpic 2  encerts: 21 / Notícies: 26 / 80.77 %\n",
    "#Tòpic 3  encerts: 29 / Notícies: 32 / 90.62 %\n",
    "#Tòpic 4  encerts: 26 / Notícies: 29 / 89.66 %\n",
    "#\n",
    "\n",
    "#Error naive bayes: 17.36 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resultat Final</b>\n",
    "\n",
    "Definició de la funció principal. Modifiqueu la funció per tal que s'ajusti a les vostres funcions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main. Es criden a totes les funcions per a la correcte execució del programa.   \n",
    "def main():\n",
    "\n",
    "    import pandas as pd\n",
    "    data=pd.read_csv('./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "    N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor. \n",
    "    words_topics=count_words_topic(data,dicc_text)\n",
    "    top_words=topNwords(data,words_topics,N)\n",
    "    feature_vectors = create_features(data,top_words) \n",
    "    naive_bayes(data,feature_vectors) #fins aquí error d'entrenament. Fem servir totes les dades.\n",
    "    \n",
    "    \n",
    "    n_fold(data, feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-7f6489286121>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;31m# Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mwords_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_words_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdicc_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File ./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv does not exist"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
